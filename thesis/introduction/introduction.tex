\chapter{Introduction}

With the increase of usage and importance of Internet services in everyday life, there also increases demand on service reliability and constant availability.
Providing these features to the services is a complex matter, involving a trade-off between user satisfaction and service resource demands.

The most common and effective method of providing reliable services is replication. Applying the replication requires only minor changes for non-replicated service to become replicated one. Thus, this method is usable both for developing new services using well known design patterns as well as transforming existing services into replicated ones.

Each replication approach relies on some group communication mechanism, usually providing strong guarantees. For example, the total-order (atomic) broadcast is a very useful for interconnecting replicas -- it simplifies the design and implementation of the replication protocols, as it allows for dividing the system into two main modules with clearly visible responsibilities.

We have chosen to implement one of the replication approaches -- one that we think is very promising.
It is called active replication. In this approach, each replica does the same work. The replicas must behave identically and therefore we require the service to behave deterministically.

Every library for replicating services should address its main purpose, i.e.\ to guarantee high availability of service as well as durability of the service work results.


{\bfseries
As part of our diploma project, we have developed a library and runtime system for efficient service replication. Our system, written in Java, implements the Paxos protocol \cite{Lam98} for distributed consensus.

JPaxos requires deterministic behaviour from the service, does not tolerate Byzantine failures and works with static group of replicas.

JPaxos supports replica crashes as well as recovering the crashed replicas, \linebreak ensures durability in spite of failures, including the failure of all replicas at once. \linebreak It provides availability of service as long as a majority of replicas are correct.
}

\section{Thesis organization}

In the opening chapter we shortly characterize articles concerning Paxos algorithm and present the most significant definitions as well as theoretical results concerning Paxos.
The second chapter provides description of the Paxos algorithm.
In chapter 3 we describe the design of JPaxos -- its architecture, modules, important data structures and threading model.
Chapter 4 is devoted to the implementation work related to the state machine replication, including snapshotting, catch-up and achieving transparency for the service.
Fifth chapter contains description of optimizations we used in order to achieve better performance of the library.
Chapter 6 describes recovery related issues, both for Paxos and for the service replication.
The last chapter concludes the thesis.

\section{Related work}

The Paxos and MultiPaxos algorithms have been proposed by L. Lamport in \textit{The Part-Time Parliament}~\cite{Lam98}. Since the article presented the algorithm in quite specific form, the Paxos protocol has been described again by Lamport in \textit{Paxos Made Simple}~\cite{Lam01}.

Since then the Paxos protocol has been described from the theoretical point of view. Improvements and their proofs of correctness were presented as well as behaviour with byzantine crashes has been characterized.

One of the first real-world Paxos application has been described in 2007 (\textit{Paxos made live}~\cite{CGR07}). The article describes certain implementation issues concerning the Paxos algorithm in the Chubby distributed filesystem developed at Google.

A year later, in \textit{Paxos for system builders}~\cite{AK08}, the Paxos protocol has been described from programmer point of view.

In 2009, and subsequently in 2010, M. Primi et al.\ released two implementations of~Paxos designed to achieve good performance. Both of these implementations have been released as open-source. The first -- libPaxos$^2$ -- is described in Primi's master thesis, the latter -- RingPaxos -- is described in \textit{Ring Paxos: A High-Throughput Atomic Broadcast Protocol}~\cite{Mar10}.

The crash-recovery model has also been the subject of many research papers. \linebreak The \textit{Atomic Broadcast in Asynchronous Crash-Recovery Distributed Systems}~\cite{rodriguez2000atomic} provides definitions concerning recovery as well as shows a method to transform any crash-stop consensus protocol to the crash-recovery one.

A survey of the crash-recovery approaches for the Paxos algorithm can be found in N. Santos technical report~\cite{Nun10}. All algorithms implemented by us are described there.

\section{Contributions}

Jan Kończak has prepared the design and implementation~of:
\begin{tightList}
  \item[\textbullet] catch-up module (Section \ref{sec:catch_up}),
  \item[\textbullet] snapshotting module (Section \ref{sec:snapshotting}),
  \item[\textbullet] service proxy  (Section \ref{sec:serviceProxy}),
  \item[\textbullet] benchmark project.
\end{tightList}

\noindent Tomasz Żurkowski has prepared the design and implementation of:
\begin{tightList}
  \item[\textbullet] unit tests for JPaxos project,
  \item[\textbullet] client manager module, 
  \item[\textbullet] network module for communication between replicas,
  \item[\textbullet] view-based recovery (Section \ref{sec:view_ss}),
  \item[\textbullet] disc writers for full stable storage recovery.
\end{tightList}

\noindent JPaxos has been developed in collaboration with Laboratoire de Systèmes Répartis at École Polytechnique Fédérale de Lausanne (Distributed Systems Laboratory at EPFL). Part of the ideas and implementations work has been done by Nuno Santos - a research assistant in the aforementioned laboratory at EPFL, working towards his PhD degree.

\noindent Nuno Santos has provided us with:
\begin{tightList}
  \item[\textbullet] a preliminary, rough code that we built upon; the code included:
  \begin{tightList}
    \item[---] the core MultiPaxos algorithm
    \item[---] failure detector and leader selection
    \item[---] basic replica, client and service implementation
  \end{tightList}
  \item[\textbullet] refactoring of batching and threading model
  \item[\textbullet] benchmarks of batching and pipelining modules
\end{tightList}

%\begin{tightList}
% \item[1.] Implementing, enhancing and fixing JPaxos
%  \begin{tightList}
%    \item[\textbullet] initial implementation 30.06.2009 (had about $13\%$ of current code size):
%    \begin{tightList}
%      \item[---] core MultiPaxos algorithm
%      \item[---] failure detector and leader selection
%      \item[---] basic replica, client and service implementation
%    \end{tightList}
%    \item[\textbullet] Renaming classes and cleaning up the code 22.09.2009
%    \item[\textbullet] Changing Benchmark project 06.10.2009
%    \item[\textbullet] Code cleanup 30.10.2009
%    \item[\textbullet] Tampering with the code 19.01.2010
%    \item[\textbullet] Improving the dispatcher thread 29.01.2010
%    \item[\textbullet] Moving failure detector from own thread to Paxos thread, tampering with the code 04.02.2020
%    \item[\textbullet] Adding support for paxos.properties file instead of hardcoded settings 05.02.2020
%    \item[\textbullet] Improving paxos.properties file 10.05.2010
%    \item[\textbullet] Code cleanup, theoretical bug fix 18.05.2010
%    \item[\textbullet] Moving catch-up from separate thread to the Paxos thread, rewrite proposer part batching, removing value from accept messages, improving retransmitter, rewriting TCP communication 22.06.2010
%    \item[\textbullet] Introducing priority queue in the Dispatcher thread, tampering with the code 24.06.2010
%    \item[\textbullet] Testing and fixes after branch merge 11.08.2010
%    \item[\textbullet] Improved batching 31.08.2010
%    \item[\textbullet] 'Improving' the TCP so that it does not work properly 30.09.2010
%  \end{tightList}
%
%  \item[2. ] Benchmarking and using JPaxos
%  \begin{tightList}
%    \item[\textbullet] Started Leader Oracle (not included in JPaxos) 22.09.2009
%    \item[\textbullet] Changes to Leader Oracle 22.01.2010
%    \item[\textbullet] Changes to Leader Oracle 04.02.2020 - 32.02.2010
%    \item[\textbullet] Modifications for improving benchmarks 07,15,26.07.2010
%    \item[\textbullet] Further improvements for benchmarks 17.08.2010
%    \item[\textbullet] Improving benchmarks again... 13.09.2010
%  \end{tightList}
%\end{tightList}


\noindent The remaining modules, benchmarks and bugfixes were the result of our joint work.


\clearpage

\section{Definitions}

In order to prevent misunderstandings and to clarify the subject of the thesis we begin from introducing
basic terms that we use in the reminder of this thesis.

\paragraph{Failure (crash)}
is a permanent lack of activity from a program. It may be caused by~programming error, lack of electricity etc.
We do not consider byzantine failures, i.e.~a~process may not misbehave in any way.

\paragraph{Catastrophic failure} is a failure of all processes at the same moment.

\paragraph{The failure model}
defines what type of failures can be tolerated by a system
\begin{tightList}[ \setlength{\leftmargin}{2\leftmargin}]
 \item[\textbf{Crash-Stop}] means that if a process failed, it failed permanently and must never be up again
 \item[\textbf{Crash-Recovery}] assumes that a crashed process may recover (i.e.\ start working again)
\end{tightList}

\paragraph{Correct process} is a process that did not crash, i.e.\ that works according to the expectations.

\paragraph{Consensus}
is a problem in distributed computing that encapsulates the task of reaching distributed agreement in the group of processes in the presence of faults.
The consensus protocol guarantees:

\begin{tightList}[\setlength{\leftmargin}{2\leftmargin}]
    \item[\textbf{Validity}] any value decided is a value proposed by some process,
    \item[\textbf{Agreement}] no two processes decide differently,
    \item[\textbf{Termination}] every correct process eventually decides,
    \item[\textbf{Integrity}] no process decides twice.
\end{tightList}

\paragraph{Instance (ballot)} is a single logical run of an algorithm. In order to decide on multiple values, many consecutive \textit{consensus instances} are executed, each identified by its \textit{ID}.

\paragraph{Value} is the value consensus agrees upon. In JPaxos, the value always consists of the client requests.

\paragraph{State machine}
is either a program, an algorithm or a protocol that can be described by its state and that can transit to other state only by receiving a command.
There are no restrictions how the state may change.

\paragraph{Deterministic state machine}
is a state machine that from the same state under the same command will always change state in the same way.
Thanks to this property, one may describe the state machine's state by the initial state and consecutive commands. 

\paragraph{State machine replication} is a method for implementing a replicated state machine, i.e.\ for maintaining multiple copies of identical state machine, possibly on separate network nodes, usually in order to gain availability of the state machine and durability of its state. State machine replication includes proper client handling and guaranteeing uniform state changes in all replicas.

\paragraph{Service}
is a program that receives requests (or commands) and executes them generating a response.

\paragraph{Deterministic service}
is a service that in a given state will always given the same response for a given command, and will always change its state to the same state.

\paragraph{Snapshot}
is the saved state of a service at a specified point in time that can be used by~the same service to restore its state to the exact state from the specified point in time. The snapshot should be in format that may be easily stored and transferred via network.

\paragraph{Client}
is the program sending requests to the service.

\paragraph{Atomic (total order) broadcast}
is a networking primitive providing send-to-all communication for which holds:
\begin{tightList}[\setlength{\leftmargin}{2\leftmargin}]
 \item[\textbf{Validity}] if a process delivers a message, it was broadcast by some process,
 \item[\textbf{Agreement}] if a process delivers a message, all valid processes will deliver it,
 \item[\textbf{Integrity}] a message is delivered only if it was broadcasted previously, and it reaches all valid processes at most once,
 \item[\textbf{Total order}] each two messages are delivered in the same order at every process.
\end{tightList}

\noindent The atomic broadcast problem is equivalent to consensus, i.e.\ if one can be solved, then the other also can be solved.

\paragraph{Stable storage}
is the memory that survives crashes. Usually stable storage denotes a~hard drive.
Sometimes also \textbf{volatile storage} name is used, to name memory that does not survive crashes, like the RAM memory.

\noindent The writes to the stable storage must be permanent contrary to the volatile memory. Thus if a hard drive is used, the writes must be synchronous.

\section{Theoretical limitations}

\paragraphNewline{The FLP impossibility result}

The consensus problem is not solvable in an asynchronous system where at least one process may crash and processes communicate by sending messages. This fact has been proved in the \cite{FLP}.

\paragraphNewline{Number of messages}

In the best case no algorithm is able to solve consensus faster than in the time needed to send one message with the value and one message not carrying value agreed upon.

Our implementation, as described in section \ref{par:bestCaseMessages}, is able to decide client commands in such time. Moreover, assuming that no network congestion occurs and no messages get lost the average time is equal to the best-case time.

However, with TCP and simple UDP it is not possible to use either multicast or~broadcast primitive; this prolongs the real time needed for sending a message, as~in the JPaxos network module \emph{broadcast} is translated to $n$ identical \textit{unicast} messages.

Other Paxos implementations do use a low-level multicast protocol for reducing communication. As an example, the RingPaxos \cite{Mar10} uses IP-multicast. The most significant gain from using multicast is scalability -- without the low-level multicast, each additional replica decreases the overall performance.

