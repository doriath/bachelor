\chapter{Paxos}

In this chapter we provide some background on the Paxos and MultiPaxos algorithm. We start with the introduction to the Paxos and MultiPaxos protocol. Then we describe the problems we encountered while implementing these protocols and performance improvements that we made.

\section{Overview}
The Paxos algorithm is used to solve consensus problem in a distributed system. Consider $n$ processes that try to decide upon the same value which was proposed by one of them. Paxos does not need a coordinator, however some process may consider itself a leader for a certain time for a specific ballot. As long as there is one leader and the majority of the processes are correct, liveness is guaranteed.

In order to simplify Paxos as well as for increased performance, the MultiPaxos protocol has been proposed \cite{Lam01}. It introduces one leader for all ballots.

The phase of electing the current leader is described in the following section. When the leader is elected, it starts proposing new values submitted by clients.

% TODO: TZ jak działa Paxos

\section{Leader Election}
\label{sec:leader_election}
\indent\par

To decide who is the current leader, our algorithm is using the \textit{view} number. This number is sent in every message and processes keep track of the highest \textit{view} received. The current leader is a process for which \textit{view $\mod$ n $=$ local id}. For example, if \textit{view} = 5 and we have \textit{n} = 3 processes then processes with \textit{local id} = 2 is the current leader.

To detect if a leader is correct, a failure detector is used. In JPaxos this is done using heartbeats sent periodically by the leader to all processes. The heartbeats are sent only when there are no other messages being sent, that is, the leader sends an \alive message to the replicas if it has not sent any message during the last $\tau_0$ time.

\begin{TODO} % TODO TZ
When a replica does not receive a message from the leader for more than $\tau_1$ time, it suspects the leader and tries to become the new leader. This phase is called a \textbf{prepare phase}. The replica process advances to the next view where it is the leader and sends a \prepare message to all. For example, if \textit{view} = 5, \textit{n} = 3 and we are process with \textit{local id} = 1, we will advance \textit{view} to number 7 -- first view where process 1 is a leader.

To become a leader, process needs to prepare the view. It sends the \prepare message to all processes with its new \textit{view}. Every process which received this message, advances its \textit{view} and responds with \prepareOK[]. When a process receives majority of \prepareOK message, it becomes a leader. Each \prepareOK message is a promise that the sender will drop all messages with lower view -- that means from old leaders or processes which do not know about the new leader.
\end{TODO}


\section{Propose phase} 

Every process keeps track of already proposed as well as decided values in a log. A log is an ordered list of consensus instances -- triple \textit{<id, view, value>}. When a process is a leader, it can start a propose phase for a new value. To propose a value, the leader creates a new consensus instance with first available id, current view and the value to propose. Then it is sends all the data to all processes in a \propose message. Every process after receiving the \propose message saves it to its local log and sends the \accept message to all. When any process receives the majority of \accept messages, it marks the proposed value as decided. The value is then passed to the upper layers.

% TODO TZ ^^^^^^

\section{Division of responsibility}

As introduced in \cite{Lam01}, the Paxos protocol tasks can be divided into three groups:
\begin{description}
 \item[Proposer] is responsible for proposing the values in correct order. In JPaxos it takes the requests from the Replica and proposes them.
 % TODO TZ ^^^^^^^^^^^^^
 
 \item[Acceptor] is a part of JPaxos that receives the \propose messages and responds to these messages according to the Paxos protocol -- i.e. when the view is not lower than current view.
 
 \item[Learner] gathers the \accept messages and if it gets the response from the majority of acceptors, it marks the value as decided. In JPaxos, it informs the Replica that a decision has been taken.
\end{description}

Our implementation also uses this division in order to make the code more readable. Every process acts as Acceptor, Learner and Proposer.

\section{Concurrent instances}
\label{subsec:concurrent_instances}
Running multiple consensus instances simultaneously is an easy way of making the MultiPaxos algorithm faster. Usually, a single instance at time is not able to use available bandwidth; running more instances makes it possible to use the network more efficiently.

Theoretically, we may have any number of concurrent instances active at the same time. We can execute some decision only after all decisions from all instances with the lower number have been executed.

In practice, the number of instances must be bounded due to following reasons:
\begin{itemize} 
  \item after leader change, \prepareOK must contain all leader's undecided instances,
  \item the network has a limited bandwidth, and conducting more instances will not increase throughput,
  \item there is no gain if we have decisions for some future instances, but the decision for an instance with lowest ID has not been taken yet (we can neither respond to the client, nor execute the taken decisions),
  \item when using UDP, more concurrent instances are causing network congestion so packets are dropped by the system and instances are decided slower.
\end{itemize}

The biggest problem connected with concurrent instances is the possibility of ordering the same request twice. Example scenarios explaining the problem are the following:
\begin{figure*}[ht]
  \includegraphics[keepaspectratio, width=\textwidth]{paxos/duplicating_messages.pdf}
  \caption{Duplicating messages - Scenario 1}
\end{figure*}
\begin{description}
  \item [Scenario] There are three replicas - $R_1$, $R_2$ and $R_3$. $R_1$ is the leader, proposes <1:B> (value B in first consensus instance). No other processes receives these messages and $R_2$ becomes the leader. $R_2$ proposes <1:C> and <2:B>. <2:B> is decided. $R_2$ fails and $R_1$ becomes the leader again. While preparing, it learns about <2:B>. It has <1:B> as a previously accepted value. The Paxos algorithm requires $R_1$ to propose <1:B> again, since its the accepted value with the highest timestamp, but this will result in the request being decided twice.
\end{description} 

Unfortunately we cannot prevent requests from being decided twice in two different instances. Because of that, the only valid solution for this problem is to remember the last executed instance for every client and not executing it again. But this requires to keep a new structure in memory as well as by every snapshot (after recovering from the snapshot, this structure has to be loaded too).

Even if we cannot prevent from deciding a request twice, we should minimise the number of such situations because they slow down the algorithm (we have to send redundant data in each message). Possible solutions to decrease the scale of this problem are listed below:
\begin{itemize}
  \item We can cache the last reply for each client, so after receiving the same request we can immediately response.
  \item Do not propose requests which have been already decided
  \item The current leader can save which requests were proposed by him (but not decided yet) and also discard them.
\end{itemize}

\paragraph{Setting the window size}
The windows size specifies how many concurrent instances can be started (it has very similar meaning to the window size in the TCP protocol). In order to use available resources and keep high responsiveness the window size must be set correctly. There is no global, generic solution: every usage needs different setting.

The most important thing is monitoring the network: its usage should be high, but no congestion should occur.
That means, on one side, the window size must be enlarged as long as the links are not full, and on the other side, it must be guaranteed that no message will be delayed or dropped because of the network traffic. As long as the two conditions hold, a speed of a single instance remains constant (putting aside CPU time).
If network is overused, the overall throughput may even stay on the same level, but the latency will increase for sure.

If the instances would be decided out of order the JPaxos performance would not be affected much. But the state machine must get the requests in proper order, so the execution of requests with greater IDs would have to be delayed. And this causes inefficient request arrival time for the service. So the number of concurrent instances should not be increased if it does not increase the JPaxos throughput.

\paragraph{Performance gains}
A single instance will nearly never use available bandwidth, so conducting concurrent instances will surely speed up JPaxos.

As already mentioned, using the concurrent instances will improve latency and throughput. Carefulness is required in order to avoid network saturation, especially the link of the leader which has to carry $2n$ messages for each instance (as opposed to 2 messages for every other link).

In full-duplex networks, concurrent instances will improve network utilisation, because when the leader is waiting for replies, the network from the leader to the others is idle, so it can be used to send the next request. 

\paragraph{Batching} is another method for improving performance and network usage. It is preferred over increasing the concurrent instances count, however joining these two methods is better than any of them alone. Batching is described in section \ref{sec:batching}, as it does not concern the Paxos protocol, but values passed to the Proposer.

\section{No operation command}

When concurrent instances optimisation is used, it is possible that leader after prepare phase will have a gap in the log. To illustrate the problem, let's analyze the scenario below.

\begin{description}
  \item [Scenario] We have three replicas $R1$, $R2$ and $R3$. Replica $R1$ is the leader and proposes <1:A> and <2:B>. Value <2:B> is decided by all replicas, but no other process receives <1:A>. $R1$ crashes and $R2$ becomes a new leader. After prepare phase, $R2$ has a gap in the log because no information about instance 1 was received.
\end{description} 

As we can see after the prepare phase, a leader can have a gap in the log. Without filling this gap, replica cannot execute and propose new values. Because of that, a new value has to be proposed in this place.

With crash-stop or crash-recovery without stable storage the value may be lost forever. That happens when decision $k+1$ is already decided, and decision $k$ is unknown to all but the leader, and the leader crashes. As only he got the value, and it has been lost -- either because he is not allowed to recover, or he is recovering without stable storage.

On the other hand, the leader change may happen due to the lost of \alive message. In this case someone has the value, and it could be used back in the voting. But there is no simple and fast method to detect this situation.

The easiest solution is to create a new request type called no-op, which is a null operation that will be ignored by the replicas. It fills the gaps but is not executed by the state machine.

\section{Log handling}

The replicated log cannot be allowed to grow forever, it must be bounded in any practical system. After any replica executes some command, it no longer needs the corresponding log entry locally. But other replicas may not have learnt the decision yet. In this case, these late replicas need to learn the decision by asking the corresponding log entry from a replica that still has it.

In JPaxos the Paxos protocol is not used for getting lost requests due to performance reasons. For this we use a special procedure called \textit{catch-up}, which is described in section \ref{sec:catch_up}.

Having the old log is needed in three cases: for the catch-up, for view change and, in the crash-recovery model, for recovery.
The catch-up needs old log in order to send them to a replica which is not up to date, once the replica requests them.
During the view change the leader requests log entries of all instances it considers to be undecided.
By recovery a replica needs the log of other replicas in order to know what has been decided at least since its crash.

Especially the latter is problematic. The view change always requires a bounded number of instances in the log. But if we assume recovery with minimal or no stable storage (as in the case of the epoch based or view based recovery algorithm), after the recovery a replica needs to recover the state from scratch. For this, it needs the whole log or complete state.

\paragraphNewline{Global Commit Point}

A command can be deleted from the log after being executed by all replicas in crash-stop and crash-recovery models with stable storage. The highest instance executed by all replicas is called a \emph{global commit point}.

But removing all instances prior to the global commit point is not enough to ensure that the log is kept bounded, because a correct replica may be disconnected from the system for some time. When communication is reestablished, the replica needs to learn about all the decisions taken in the meantime. If one replica crashes, the other replicas have no way of knowing that it is a crash and not a disconnection, so they would have to keep the logs forever. And the replica can stay crashed for arbitrary long time.

Therefore using snapshotting is the only acceptable solution in JPaxos.

\paragraphNewline{Snapshotting}

A replica needs the old log entries in order to bring its state up to date. But instead of executing all decided requests the replica may transfer the state of the service from other replica. To transfer the state of the service some assumption must be fullfilled: the service must be able to save its state, and be able to update itself to such state.

The service can create a \textit{snapshot} -- arbitrary data that can be parsed by the service to change its state to the state identical as in the moment when the snapshot was created.

The mechanism of managing such snapshots is called by us \textit{snapshotting}.

In order to be able to remove stale log entries snapshotting must to be implemented. It also speeds up the recovery and process of filling missing instances.

The size of a snapshot is of course bounded. If we create a new snapshot as the log size reaches some limit the size of the log is bounded as well, so no structure is allowed to grow forever.

When a new snapshot is created the old log entries are truncated. If a replica that is missing old logs rejoins, the most recent snapshot is transferred first and then all the decisions since the snapshot are sent to complete the updating procedure.

The most problematic part is the Service -- if it does not implement creating snapshots or does not provide them often enough, the log will grow forever.

Snapshotting is described in details in the section \ref{sec:snapshotting}.

\section{Skipping redundant messages}
Reducing the number of messages is one of the most efficient optimisations. In JPaxos a process does not send messages to itself, also the proposer does not send \accept messages to anyone.

It has been proven for the Paxos consensus algorithm that it minimises the number of messages carrying a value to be agreed upon (the client request in our case). The value is sent only once to each replica (not counting retransmission due to message loss). As for additional messages, their count varies among certain implementations. Some of messages proposed in the typical implementation may be easily skipped without violating guarantees or adding any assumptions.

\paragraphNewline{Using {\normalfont\propose}as {\normalfont\accept}}

Another optimisation is merging the role of \accept and \textsc{Propose}. As long as acceptors and proposes do not share the log this is not allowed. But if their knowledge is identical and the leader proposed some value, then he must accept it as well. In most solutions one node acts as proposer and acceptor, and they do share the log.

So in JPaxos each \propose is simultaneously an \accept message. Notice that if the system consists of three nodes every process decides on the value by receiving a valid propose -- it knows that the leader and itself
accept the message. Normally the leader process has to first send \propose to all and later \accept to all. By this optimisation only half of messages is required to be sent.

\paragraphNewline{Sending to itself}

As stated before, Paxos defines three types of roles: proposers, acceptors and learners. Processes or some of their parts act as one of them. In order to communicate they must send messages to each other.

In our program every node consists of a (probable) proposer, acceptor as well as learner. In this case there is no need to send messages to self. Especially all send-to-all commands may be replaced by send to others command, as long as the communication with self will be done internally. This skips one \accept and one \propose sent by the leader, and one \accept by all other processes.

This does not influence performance much, however it is recommended as it decreases the use of system resources (especially the networking stack). It also skips a few system calls.

\paragraphNewline{Minimizing the count of messages carrying the value}

Most of the articles about Paxos state that the \accept message carries the value (request from client).
In such case we can accept on the value even without receiving a \propose message.

The value usually is bigger than a normal message of JPaxos. Because of that we want to minimize number of messages which contain the value.
In JPaxos the \accept message does not carry the value. Before a process can send the \accept message it has to receive the \propose message. The process decides upon a value if it receives a \propose and majority of \accept messages.

Waiting for the \propose message might cause additional overhead, but usually the \propose will be delivered prior to the majority of \accept messages.
Each process must receive one \propose[], but multiple \accept messages. If the \accept message does not carry the value, significantly less network bandwidth is used by a single instance.


\paragraphNewline{Best-case messages}
\label{par:bestCaseMessages}
So, in the normal operation in JPaxos:

%TODO: TZ: Jeśli masz sprawnego inkscape'a, narysuj proszę do tego prosty rysunek best case 
% mi nie tylko vim'a popsuł niedokończony update systemu, inkscape też narzeka na brak jakiegoś .so
% chodzi o diagram odpowiadający komunikacjom z tabelki poniżej

\begin{table}[h]
\begin{tabular}{rccccc}
          & \begin{tabular}{c} How many \\ processes send \\ the message \end{tabular} 
          & \begin{tabular}{c} Count of \\ messages \\ sent \end{tabular} 
          & Time 
          & \begin{tabular}{c} Message \\ type \end{tabular}
          & \begin{tabular}{c} Carries value \\ (client requests) \end{tabular}
          \\
 Proposer & $1$                     & $n-1$             & $n$  & \propose & yes    \\
 Acceptor & $n-1$                   & $n-1$             & 1    & \accept  & no
\end{tabular}
\end{table}

We can see that in order to decide the process needs to send the \propose[] once, and later each non-leader process sends the \accept to others.

For the comparison of consensus algorithms usually time is defined in a specific way: $n$ denotes the time needed to send a message containing the value to be agreed upon, and $1$ is the time required to send any other message. This is motivated by the value size -- usually the value is significantly bigger than any protocol message, so it is important to differentiate the messages carrying value and all the others.

Other implementations consider using bounded learner count -- only some processes are selected as the learners. For example only one process plays the role of a learner -- that means the acceptors send the \accept messages only to this process. An additional message, \textsc{Decided}, is sent by the learner to all as the ballot has finished. Thanks to this modification no reliable broadcast is needed, but also an additional phase of communication must occur. If only some processes act as learners, it is important to ensure that there is at least one learner that didn't crash, otherwise the protocol would not guarantee liveness.

An interesting approach is presented in the RingPaxos (see \cite{Mar10}), where IP-multicast is used for all broadcasts, what significantly increases the scalability and performance.
