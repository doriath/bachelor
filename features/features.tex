\chapter{Implemented Features}

In this chapter we present the implementation details of some non-trivial modules.

\section{Unique request IDs}

A request must be distinguishable from any other request. Only this guarantees that we will be able to say if a request is retransmitted, or it's a new one. A common method is to split the request identification in two parts: the client ID and a sequential number. As far as assigning a sequential number is trivial, a method providing unique client IDs must be chosen carefully, especially taking the crash-recovery model under consideration.

The client ID has to be unique for two different clients. To guarantee this, replicas must be responsible for granting IDs to clients, because a client has no knowledge of other clients ID.

Below we present some of the possible methods for solving this problem:

\paragraphNewline{Based on the number of replicas}
Each replica can grant numbers % 
incremented by a number of replicas, starting from ID of a local replica. For example, if we have three replicas, replica with ID 0 can grant numbers 0, 3, 6, 9, \ldots{} and replica with ID 2 can grant 2, 5, 8, \ldots{} . It guarantees that two different replicas will not grant the same ID. This solution works with the crash-stop and crash-recovery with stable storage. This will not work when replica can recover from crash without stable storage, because it doesn't know what was the last granted ID.

\paragraphNewline{Time based}
This is a different approach -- we use system time as the source of unique numbers. To each client ID we are also adding information when replica was started (in milliseconds).

For example, if a replica has been started at time $t$, it will grant for new clients IDs $(t + $\textit{localId}$)$, $(t + $\textit{localId}$ + n)$, $(t + $\textit{localId}$ + 2n)$ \ldots

This method assumes that the local system timer is not set back into past and also that the replica will not recover in the same millisecond. If these assumptions are fulfilled, then unique IDs are guaranteed also in the crash-recovery model without stable storage. This approach is currently default in our library.

\paragraphNewline{View based}
Because time-based assumptions sometimes cannot be guaranteed, another approach for the crash-recovery model may be considered. Instead of adding information about start time, we can add the current view number. In each view, there is exactly one leader -- so after the prepare phase this replica can grant clients IDs as pair (view, sequence number). Two different replicas cannot start the same view.
Also we have to guarantee that only the leader replica after prepare phase can grant new client IDs.

This approach will also work by using the epoch numbers -- if a leader crashes, the view must be increased by his recovery. % TODO TZ


\section{Catch-up}
\label{sec:catch_up}
%TODO TZ should it be here ?

\paragraphNewline{Motivation}
% TODO TZ rewrite - reader can't understand what catch-up is from this text 
Paxos must guarantee that every valid replica will eventually decide. Most papers about Paxos state that if a replica notices the lack of activity for some instance, it must send a new proposal and eventually decide. But by using view for all instances, this would mean often view changes -- and this contradicts performance.

Therefore a different approach is used by us. Notice, that if the leader is correct, it will decide. If it has crashed a new leader will be elected. If the leader is correct, and no activity about an instance is noticed, that means the value is already decided and the replica is in the minority that does not know about the decision.

In such case, instead of forcing a new proposal, we use the catch-up module.
The catch-up has to download all missing decisions. This solution is faster than restarting the ballot if it is already decided and it also does not need a view change.

\paragraphNewline{Recovery}

Catch-up module is also used by recovering the replica -- as a tool for downloading all decided instances up to the required state.

But there is one drawback of using catch-up in recovery -- for the recovery process needs to know also instances yet undecided.  However, if the system is operational, after a short time all required instances will be decided. If the system is not operational the recovery cannot converge. So downloading only decided instances is correct, but may cause small delay by recovering.

\subsection{Log-based vs state-based recovery}
\label{subsec:log_based_state_based_recovery}
A replica can catch-up either by copying the missing decisions or by a combination of transferring the state plus the most recent decisions.

In the rest of this paper \emph{log-based recovery} refers to the first option while \emph{state-based recovery} refers to the second option.

One should use of course the method achieving better performance. But which method is faster depends on the particular characteristics of the system:

\begin{description}
  \item[Size of the state] For services with large states  it is better to avoid transferring the state as much as possible. On the other hand, if the service state is small, transferring it might be better. In the extreme case, the service state is of a similar size or smaller than a single command. In this case, it would always be better to transfer the state.

  \item[Execution overhead] Executing commands takes some processing time which is not required when transferring the state. Therefore, even if the service state is bigger than the corresponding commands, it might be faster to transfer the state.

  \item[Bandwidth available] With limited bandwidth, the fastest  method will likely be the one that transfers less data. Otherwise, the execution overhead will likely be the dominant factor.
\end{description}

\paragraphNewline{State-based recovery time}

State-base recovery has the following steps:

%\vspace{-0.5em}
\begin{center}
  %\footnotesize
  \begin{tabular}{ll}
    Serialisation               & state is transformed into a sequence of bytes \\
    Transfer                    & state is sent via network \\
    Deserialization             & state is rebuilt at destination \vspace{0.5em} \\
    Residual log-based-recovery & recovery from snapshot process is made \\
  \end{tabular}
\end{center}
%\vspace{-0.5em}

The first three tasks may be done concurrently. Assuming that the fourth task is comparatively fast, we can consider that the total recovery time is the maximum time it takes to do each of the first three tasks.

\paragraphNewline{Log-based recovery time}

Log-based recovery is done by sending the commands and executing them at the replica. Once again, sending and executing can be done concurrently, so the total duration is the max of the time it takes to transfer all the updates or to execute them.

\subsection{Conditions for starting and finishing the catch-up}
\label{subsec:conditions_for_starting_and_finishing_the_catch_up}
Many possible rationales may be used as indicator when the process should begin the catch-up. Also conditions needed for the stop process are a significant issue.

\paragraph*{Starting catch-up}
Activating the mechanism should not happen too early, that is when Paxos itself is still able to take the decision -- target of catch-up is to complement decisions missing due to packet loss. On the other hand late initiating leads to delaying command execution, and thus significant performance loss.

Most common events used to initialise the algorithm are:
\begin{itemize}
  \item No traffic for the ballot during a particular period of time (\textsc{timeout})
  \item An instance with higher ID (i.e. newer instance) has already been decided \\ (\textsc{higher instance decided})
  \item Instance with ID higher than $\alpha$ has been started, and the implementation ensures that only instances up to $\alpha$ are started as $\alpha-ws$ stays undecided (\textsc{window})
  \item Activate periodically the mechanism (\textsc{periodical})
\end{itemize}

These methods let the replica suspect, that a locally undecided instance has already been decided, so its log state is not up to date. It's good to use more than one of them, as each method is sensitive for a particular scenario, perhaps performing bad otherwise.

Simplest, but not efficient for most uses is the \textsc{periodical} method -- it guarantees that from time to time the replica will be up to date.
However the method used too often may cause bandwidth and processor consumption, too rarely -- having old state for unacceptably long time.

The \textsc{timeout} event is caused either if problems with network occur, or if the \accept messages were lost -- otherwise current leader and other replicas would generate traffic for the ballot. The replica should try to learn the state of voting in such case, otherwise as long as no new proposal will arise, the replica will have an old state.

\textsc{higher instance decided} means that a voting started later has already finished. We may assume the previous voting took similar amount of time -- that is, it should be already finished. Message loss or varying latency can also cause the situation that a newer consensus is decided as the older is still in progress. To avoid this situation the method may be extended to: if the number of newer decided instances exceeds a constant value, then the catch-up should be initiated.

This method, as well as the method described earlier, may be used in case when operations on state machine require long processing. The risk of loosing small bandwidth for catch-up may be small enough compared to earlier arrival time of resource-consuming task.

The \textsc{window} method is very useful because of its assumption: the algorithm uses a window of the maximum size $ws$. If the condition that all decisions outside the window must be decided is violated on one replica, it means that the other replicas, at least the current leader, have already decided the instance. This means no new messages concerning the ballot will emerge, and catch-up is necessary.

A different problem is when a replica doesn't know that the instance already exists (for example, a short-time netsplit or bad luck caused all messages for the replica to be dropped). In this case, the catch-up should be started as well. 
Either periodic requests must be sent, or additional information must be acquired. It should be emphasised, that as any newer ballot starts, the replica is informed about the existence of missing instance. 

Although this is not critical, it prevents state machine from executing the command until a new instance emerges. That means performance loss, as the old command must be caught-up and executed before the new.

In our system, replica is notified about the highest instance ID from the \alive message from the leader. The leader is attaching this number to every \alive message sent. This guarantees that every replica learns what instances already exist within the failure detector timeout.

\paragraph*{Stopping catch-up} As the algorithm runs, the state of Paxos may not be stable, i.e. new ballots may start. Therefore selecting the moment when catch-up should be deactivated is not trivial.

One method is to calculate conditions a priori (for example the IDs of missing instances), and continue the process as long as needed. But this can easily lead to a constant switching on and off the catch-up -- voting for new instances may be faster than catching up, so as the predefined conditions are met, new event already caused catch-up activation.

The better solution is to determine dynamically if catch-up is still needed. A method that seems most convenient for that is checking if all instances outside the window are already decided. Stopping earlier is bad, as we know that at least the current leader must have decided for the missing instances, and that means we will not get the value via the Paxos algorithm.

Finishing the catch-up process later, that is requiring any messages in the window to be decided, is also not a good solution -- unless we have some additional knowledge that the instance has already been decided (for example, the leader's last uncommitted instance ID is sent in \propose or \alive messages).

Problem arises, if there really were decided instances inside the window. If there are no new ballots, arrival time of these requests will be delayed. In our opinion, the problem is not that severe, especially if the window size does not exceed reasonable size.

\subsection{Transport protocol}
\label{subsec:transport_protocole}
As totally separate from Paxos, the catch-up may use different transport protocol than the consensus algorithm.
Choice of the transport layer must take into account characteristics of this mechanism as well.

Both TCP and UDP may be used -- both having their pros and cons, presented in table below

% TODO TZ
\begin{center}
\begin{tabular}{>{\raggedleft\hspace{0pt}}m{0.47\textwidth}m{0.47\textwidth}}

\multicolumn{1}{c}{ \textbf{TCP} }& \multicolumn{1}{c}{ \textbf{UDP} } \\ 
automatic retransmission guaranteed by protocol &
                                retransmission must be implemented manually \\ 
flow control provided by protocol & no flow control, it \emph{is} easy to congest network \\
big messages automatically fragmented, mer\-ged and managed &
                                splitting and handling big messages must be~self implemented \vspace{0.5em} \\

request cannot be changed at retransmission &
                                request can be changed each retransmission \\
default retransmission time & custom retransmission time \\
another message may be sent once the previous was delivered &
                                new datagram may be sent anytime, no matter if the previous reached the target \\

\end{tabular}
\end{center}

When catch-up is needed,  our network must have (probably high) message loss. That means the catch-up messages may also get easily lost.

We have noticed, that it is more efficient to use UDP for all smaller messages -- if the package gets lost, we're retransmitting it with updated query. Sometimes UDP message retransmitted even twice is faster than one TCP. In TCP, the timeout for the ACK message grows automatically - and that may cause big delays. During experiments, with 30\% message loss, transmitting a message using TCP that is smaller than the UDP-datagram size took even more than 4 seconds % are these measurements valid for the current implementation or there are result from EPFL time
to reach the other side.

In UDP, a single message delay or loss does not block communication to the replica -- but in TCP it does.
In this case no new catch-up query may be sent to this replica, as long as the previous will be processed. The request cannot of course be changed. It means that once we got the response, the core protocol may have already missed another instances to catch-up.

\subsection{Requirements}
\label{subsec:catch_up_requirements}
The speed and resource consumption must be correctly balanced.

In theory, catching-up can be delayed any finite time. On the other hand, it is important for view changes and for bounding the size of the log. A view change will take longer if the new leader is missing many requests. Since during a view change no new requests are being satisfied, we need to keep its length as short as possible. Additionally, if a replica doesn't know instance $i$, it cannot execute any request higher than $i-1$. This also means that it cannot remove entries from the log.

In practical approach most significant requirements for catch-up are: good performance and small resource usage. For sure catching up should not slow down the core protocol.
Good performance demand has one main reason: arrival time of the tasks for state machine is dependent on being up to date. If catch-up would get information too late, the state machine would get long list of, possibly high CPU-consuming, tasks at once, instead of doing them in spare time before.

Therefore, it is desirable to catch-up as often as possible, but without slowing down the service significantly.

It should be noticed, that only the followers will have to catch-up, since the primary learns of all previous decisions on a view change and then it is the one proposing new values and leading the protocol.

\subsection{Catch-up algorithm}
\label{subsec:catch_up_algorithm}
The catch-up algorithm should contact other replicas and copy the decisions that are not known. The question when this should be done is discussed in Section \ref{subsec:conditions_for_starting_and_finishing_the_catch_up}. Here the main idea of JPaxos catch-up algorithm is presented.

To activate the catch-up, we use \textsc{window} and \textsc{periodical} methods (see Section \ref{subsec:conditions_for_starting_and_finishing_the_catch_up}). The leader also sends in each \alive message the information about highest started instance.
This ensures that the replica will catch-up eventually, even if there are no new requests, and that a replica never stays too much behind the others, at most \textit{ws}.

There are three messages that may be sent during the algorithm: \catchUpQuery[], \catchUpResponse and \catchUpSnapshot[].
\begin{description}
 \item[\normalfont\catchUpQuery -] a request for missing instances. It carries a list of missing ID's and may have one of two flags set: the first flag indicates if this was a periodical catch-up, second one -- 
 if we want to get the last snapshot, not the missing log fragment. % TODO TZ ?
 \item[\normalfont\catchUpResponse -] response sent for every received request. It has a list of decided instances for requested numbers. The list may be empty. This message may have two flags: 
 either if the query had periodical flag set % TODO TZ ?
 , or an indicator that the replica doesn't have an old enough log, and 
 state transfer is the only possibility. % TODO TZ ?
 \item[\normalfont\catchUpSnapshot -] if another replica asked for our last snapshot, this message is sent. It contains only responders last snapshot. % TODO TZ ?
\end{description}

The list describing missing instances is constructed by replica in straightforward way: it contains all undecided instance numbers plus the (highestID+1) as the first instance, the replica has no knowledge about. The responder sends therefore all decided instances from the list and all decided instances that are higher or equal the additional number.

In order to make the messages smaller, a trick has been used: the list contains two lists. One called a range list and the other an instance list.
The range list contains intervals we miss, while the instance list -- single numbers. For example, if we would miss instances 1,2,4,6,7,8,9,11, and the highest instance we know is 12 (state decided) our lists would look like:
\begin{quote}
$[\langle1,2\rangle; \langle6,9\rangle]; [4,11,13]$
\end{quote} 
Notice the number 13 -- it's the first we have no idea of existing, as mentioned above.

Catch-up works as follows:

\begin{algorithmic}[1]
  \REPEAT
    \STATE \label{alg_1_a} Choosing target replica
    \STATE Creating list of missing instances
    \STATE Send a \catchUpQuery
    \STATE \label{alg_1_b} Wait for timeout or for response
  \UNTIL{\label{alg_1_c} catch-up succeeds}
  
  \vspace{0.5em}
  
  \STATE \textbf{upon} receiving \catchUpQuery \textit{query}
    \IF{\textit{query} has snapshot flag set}
      \STATE Get last snapshot
      \STATE Prepare \catchUpSnapshot message
      \STATE Send the message
    \ELSIF{requested instances already not in log}
      \STATE Send \catchUpResponse with \textit{snapshot} flag
    \ELSE
      \STATE Gather all decided instances
      \STATE Send \catchUpResponse
    \ENDIF

  \vspace{0.5em}
  \STATE \textbf{upon} receiving \catchUpResponse \textit{response}
    \IF{\textit{response} has \textit{snapshot} flag set}
      \STATE Prepare \catchUpQuery with snapshot flag
      \STATE Send the query
    \ELSE
      \STATE Merge received log (if any)
      \STATE Wake up the catch-up loop (line \ref{alg_1_b})
    \ENDIF
  
  \vspace{0.5em}
  \STATE \textbf{upon} receiving \catchUpSnapshot \textit{snapshot}
  \STATE Check if \textit{snapshot} is newer than current one
  \STATE Replace the current snapshot with \textit{snapshot}
  \STATE Truncate logs (to stop catch-up)
  \STATE Wake up the catch-up loop (line \ref{alg_1_b})

\end{algorithmic}

\vspace{1em}

In line \ref{alg_1_a} a best replica is chosen. We've implemented it as follows: a rating for each replica is kept. When sending a message, the rating decreases; when receiving response it rises. If an empty response is received (except for the \textsc{periodic} mode), we request asking the leader next time. The best replica for us is a follower with the highest positive rating, or the leader if all followers have negative rating.

The line \ref{alg_1_c} executes a predicate checking if the catch-up shall finish (see Section \ref{subsec:conditions_for_starting_and_finishing_the_catch_up}).

\section{Snapshotting}
\label{sec:snapshotting}
As presented before, the support of storing and transmitting state of replicated machine is a very useful and important part of a replica. In practical systems it is necessary -- it allows log truncating, faster recovery and catch-up.

\subsection{When to snapshot}
\label{subsec:when_to_snapshot}
Snapshots are needed in two cases: to enable catch-up and recovery, and to truncate the log. For both uses there is a different \textit{best moment} for making snapshot.

As for recovery, we get the best recovery result if the snapshot would be done every executed order.
This, of course, is not the solution. It would completely decrease the throughput -- snapshot creation is resource-consuming.

For the catch-up the moment when another replica requests snapshot is the best for creating one. 
However some services cannot create snapshot anytime. So demanding a snapshot from the service is not acceptable.

For truncating the log the snapshot should be created periodically, without any additional requirements.

Therefore we assume the snapshot should be made when the cost of catch-up from the log becomes bigger than the cost of catch-up from the snapshot.

Please note, that catch-up time also includes time of state/log transfer.

\subsection{Replica vs. service responsibility}
\label{subsec:replica_vs_state_machine_responsibility}
There are two approaches to the problem who is in charge for the initiating of snapshot creation. Either the JPaxos \texttt{Replica} module issues a snapshot, or the service chooses an appropriate moment.

Embedding this functionality into JPaxos would surely provide more secure work (the service may simply not deliver the snapshots, which means forever growing log). It is easier for the \texttt{Replica} to measure the size of messages that should be transferred in order to catch-up (or recover).

However, JPaxos does not know anything about the service. We can also assume, that the service is not aware of network conditions. Therefore choosing the proper moment (when the cost of log-based catch-up becomes more expensive than the state-based catch-up) is impossible for both. It is clearly visible, that service is better informed -- it knows not only the size of requests, but also it may estimate the size of its current state and resources that are needed to execute all commands from the log.

The latter is most significant difference: JPaxos has no idea how long the log execution from previous snapshot would take. It seems possible to estimate this: measuring time between a request and reply to that request might solve the problem. But such estimate can give mistaken results, especially in a multi-process environment. If another process is consuming CPU, or the replica waits a long time for granting some resources (like an access to a file or even a printer), the estimate surely will not reflect the real value.

Below we present a table showing in compact way the state of knowledge needed to select the best moment for a next snapshot:

\begin{table}[h]
\small \centering
\begin{tabular}{r|c|c|}
\cline{2-3}
 & Service & JPaxos \texttt{Replica} \\ \hline 
\multicolumn{1}{|r|}{Size of requests} & known & known \\ \hline
\multicolumn{1}{|r|}{Size of state } & known & estimate \\ \hline
\multicolumn{1}{|r|}{Log execution time} & good estimate & poor estimate \\ \hline
\multicolumn{1}{|r|}{Time for sending message} & unknown & estimate \\ \hline
%\multicolumn{1}{|r|}{ × } & × & × \\ \hline
\end{tabular}
\caption{Comparing knowledge of the service and the \texttt{Replica}}
\vspace{-1em}
\end{table}

\subsection{Our approach to snapshotting}
\label{subsec:our_approach_to_snapshotting}
Snapshotting, as described above, may be done in a variety of ways. Moreover how often the snapshots are made depends on the implementation. Here we give some main clues how the snapshotting is implemented.

The decision who orders a snapshot creation has been left to the future user of the library. To achieve this, some assumptions has been taken -- mainly
concerning the architecture of the service.

The service must implement three methods: \texttt{askForSnapshot}, \texttt{forceSnapshot} and \texttt{up\-date\-To\-Snap\-shot}. Also it is required to implement adding and removing snapshot listeners -- objects that implementa the \texttt{onSnapshotMade} function. When a snapshot is made on the state machine, method \texttt{onSnapshotMade} with the snapshot as a parameter must be called on all snapshot listeners.

Replica measures the size of the log after every n-th instance, and an calculates average size of the snapshot basing on previous ones. By every log measurement a ratio is calculated: $\frac{ \text{log size} }{ \text{snapshot estimate} }$. As the ratio exceeds one constant, method \texttt{askForSnapshot} is called. After another constant, \texttt{forceSnapshot} is executed.

There are several approaches possible on who decides the proper time for the snapshot:
\begin{description} 
 \item[State machine only] -- service ignores the functions \texttt{askForSnapshot} and \texttt{forceSnap\-shot} and does the snapshot on its will.
 \item[Using replica calls as hints] -- service takes under consideration \texttt{askForSnapshot} and \texttt{fo\-rce\-Snapshot} functions, but decides itself when to do snapshot
 \item[Balanced responsibility] -- service uses both \texttt{askForSnapshot} and \texttt{forceSnapshot}; the first as a hint, the latter treats as an order
 \item[Replica only] -- each time \texttt{askForSnapshot} is called, the state machine does snapshot
\end{description}

Snapshotting requires also additional data exchanged between replica and service: the state machine must know the request number, in order to let snapshot identification in replica. For example, if the snapshotting would be done completely on service's side, how would the replica know after which command the snapshot was taken.


\section{Batching}
\label{sec:batching}
Batching means using the same consensus instance to order several requests.

This optimisation does not need any changes to the Paxos algorithm, only requires that requests from one instance can be ordered deterministically by each replica, so that replicas know the order in which they should execute the requests. That is no problem, as a byte stream is already ordered -- so each replica will decode the requests the same way.

To achieve multiple requests in the same instance an additional activity must be taken up by the proposer and later, by replica, just before execution.

The proposer needs to \textit{stick} together the requests, and from that moment on they are treated as one. The other part, dividing it again into requests, should be done after the decision has been taken.
\begin{figure}[h]
\includegraphics[keepaspectratio, width=\textwidth]{features/batching.pdf}
\vspace{-1em}
\caption{The batching mechanism}
\vspace{-1em}
\end{figure}

As the count of concurrent instances is bounded (see subsection \ref{subsec:concurrent_instances}), without batching every client needs to wait for his own instance. With batching many client requests may be \textit{packed} in one instance. That, on one side, increases the throughput and decreases the delay.
On the other side, the message size is getting bigger -- and that means slower voting.

Here arises a question: how many requests to batch? One boundary is quite obvious -- a serialized message should not be bigger than the transport protocol frame size. As for UDP the size is about 65kB, but for TCP stays unbounded.

The network bandwidth is the upper bound of peformance. Increasing the size of the instance value does not change the CPU time needed to decide on this instance. So the best performance is achieved as the batching size is set to the minimal size when all available bandwidth is used.

Another problem concerns the proposer: should there be a lower limit of requests in one instance? Doing so implies waiting for new client requests to come. Not waiting means sometimes severe throughput loss.

A new request may come a long time after we begun gathering the requests for batching -- and that would cause an unacceptable delay. So if we choose to wait for new client requests, we need to set up a timeout after which we stop waiting for the new requests and start deciding the requests we manged to gather.

If we decide not to wait at all we are packing all client requests we got until now into a single request (if it fits, of course). Such approach guarantees good behaviour if the system state is not changing. Under load  we are usually unable to pack all requests we got into single instace, while with small amount of requests no delay by packing decreases the latency.

Both the batching size and the timeout for gathering requests in a batch is configurable in our library.

\section{Service Proxy}
\label{sec:serviceProxy}

Some operations on services require proper arguments. Should a service be responsible for that? Snapshot cannot represent the state after executing any request, but only after executing the last request in any instance. Should a service take care for that?

This is not desired. We want to provide as much transparency as possible, without forcing the service to remember redundant data.

\texttt{ServiceProxy} is a module that translates JPaxos instances to service requests and takes care for all the requirements concerning snapshotting. On the replica side, interaction with the service through \texttt{ServiceProxy} is instance-oriented and convenient. On the service side, the interaction with clients is as transparent as possible.

To achieve this the \texttt{ServiceProxy} keeps a list of responses for all requests since the previous snapshot as well as some information required later to properly restore a
request sequential number % one phrase ?
from a snapshot.
As the replica provides a snapshot, this module adds to the snapshot required data.

Overhead caused by this proxy should be smaller than the difference in speed of the service if the service had to be aware of instances. The difference is size of the snapshot depends on the state size, but usually the state is much bigger than a few responses required to provide transparency.
Of course, this does not release the service from responsibility of providing valid snapshots. The service still must provide the corresponding 
request sequential number % one phrase
for the snapshot it creates.
