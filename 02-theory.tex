\clearpage

\section{Definitions}

In order to prevent misunderstandings and to clarify the subject of the thesis we open it with definitions of
basic % TODO lepszy odpwoednik 
terms we use.


\paragraph{Consensus}
is a problem in distributed computing that encapsulates the task of group agreement in the presence of faults.
Consensus guarantees:

\begin{tightList}[\setlength{\leftmargin}{2\leftmargin}]
    \item[\textbf{Validity}] any value decided is a value proposed by some process,
    \item[\textbf{Agreement}] no two processes decide differently,
    \item[\textbf{Termination}] every correct process eventually decides,
    \item[\textbf{Integrity}] no process decides twice.
\end{tightList}

\paragraph{Instance (ballot)} is a single logical run of an algorithm. In order to decide on multiple values, many consecutive \textit{consensus instances} are executed, each identified by it's \textit{ID}.

\paragraph{State machine}
is any program, algorithm or protocol that can be described by its state and that can transit to other state only by receiving a command.
There are no restrictions how the state may change.

\paragraph{Deterministic state machine}
is a state machine that from the same state under the same command will always change state in the same way.
Thanks to this property one may describe the state machine's state by the initial state and consecutive commands. 

\paragraph{Service}
is a program that receives requests -- or commands -- and executes them generating a response.

\paragraph{Deterministic service}
is a service that will respond in given state for given command always with the same response, and will always change the state to the same state.

\paragraph{Client}
is the program sending requests to the service

\paragraph{Atomic (total order) broadcast}
is a networking primitive providing send-to-all communication for which holds:
\begin{tightList}
 \item[\textbullet] a message reaches all valid targets exactly once,
 \item[\textbullet] each two messages are delivered in the same order at every receiver.
\end{tightList}

\noindent Consensus is highly related with atomic broadcast. If one can be solved, then the other also can be solved.

\paragraph{Failure (crash)}
is permanent lack of activity from a program. It may be caused by programming error, lack of electricity etc.
We do not consider byzantine crashes, i.e. a process may not misbehave in any way.

\paragraph{Catastrophic failure} is a failure of all processes in the same moment -- there is a moment when all processes are crashed.

\paragraph{Crash model}
defines how the crashed processes may behave.
\begin{tightList}[ \setlength{\leftmargin}{2\leftmargin}]
 \item[\textbf{Crash-Stop}] means that if a process failed, it failed permanently and must never be up again
 \item[\textbf{Crash-Recovery}] assumes that a crashed process may recover (id est start working again)
\end{tightList}

\paragraph{Stable storage}
is the memory that survives crashes. Usually stable storage denotes hard drive.
Sometimes also \textbf{volatile storage} name is used, to name memory that does not survive crashes, like the RAM memory usually used for program data.

\noindent However, the writes to the stable storage must be permanent, so if a hard drive is used, the writes must be synchronous.

\section{Theoretical limitations}

\paragraphNewline{Infeasibility in asynchronous model}

The consensus problem is not solvable in asynchronous system where at least one process may crash and processes communicates by sending messages. This fact has been proved in FLP impossibility proof \cite{FLP}.

Therefore some assumptions concerning time must be made. In this thesis we assume that if a message is not lost, it will reach its target in finite time.

\paragraphNewline{Number of messages}

In the best case no algorithm is able to solve consensus in time less than $1n+1$, that is one message with the value and one message not carrying the any dataload.

Our implementation, as described in \ref{par:bestCaseMessages}, is theoretically able to decide messages in $n+1$ time. Moreover, assuming no network congestion and message loss the average time is equal to the best-case time.

However, with TCP and simple UDP it is not possible to use either multicast or broadcast primitive; this prolongs the real time needed for sending a message, as in the lowest network module \emph{broadcast} is translated to $n$ identical \textit{unicast} messages.

Other Paxos implementation do use multicast for reducing communication. As an example, the RingPaxos \cite{Mar10} uses IP multicast. Most significant gain from using multicast is the scalability -- without the multicast each additional replica diminishes the performance.

